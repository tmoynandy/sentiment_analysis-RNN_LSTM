{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis:RNN_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BuclukcV9MsG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data & minimal pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "5f9RfW9R5l1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data description** : Sentiment analysis on tweets during first GOP debate 2016\n",
        "\n",
        "**Data provided by** : [Data for Everyone](https://www.figure-eight.com/data-for-everyone/)\n",
        "\n",
        "**Dataset Link** : [Here](https://drive.google.com/open?id=19aQHO2TImE0fdvk_m1G0cBvS1lJC-jNc)\n",
        "\n",
        "**Why not whole twitter data**? \n",
        "Because political debates have polarised and more strictly classifiable sentiments, the property of which can help in bettter training.\n",
        "\n",
        "(Machine learning is more about intuition than code)\n",
        "\n",
        "**Description** : According to original dataset provider \n",
        "\n",
        "```\n",
        "We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "s0kTT6j95NNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "287f0cb7-91ec-44ea-e661-ce48e3798073"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G4ZJV3np7LXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eebcd58e-eea9-463e-9ad0-e50819e545a0"
      },
      "cell_type": "code",
      "source": [
        "#making all necesary imports\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XQzFvlAP78oh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/Colab Data/Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upCAZpQq8RSM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. \n",
        "\n",
        "After that, I am filtering the tweets so only valid texts and words remain. Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "metadata": {
        "id": "7vBltO8V8JrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2bc8b41e-29f0-463c-b5bc-d57034d612db"
      },
      "cell_type": "code",
      "source": [
        "data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "print(data[ data['sentiment'] == 'Positive'].size)\n",
        "print(data[ data['sentiment'] == 'Negative'].size)\n",
        "\n",
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4472\n",
            "16986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QhAAHm5D9XaD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Defining our LSTM RNN Model"
      ]
    },
    {
      "metadata": {
        "id": "1RVKA86O8rOe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, I compose the LSTM Network with softmax as an activation"
      ]
    },
    {
      "metadata": {
        "id": "2qTD4B748ZPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "68cef91c-f6d2-4940-f5c5-5aeab22e2d12"
      },
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 28, 128)           256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 28, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 511,194\n",
            "Trainable params: 511,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YBdv34Az853D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that **embed_dim**, l**stm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. "
      ]
    },
    {
      "metadata": {
        "id": "iWWzksDA9Dh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Declaring the train & test "
      ]
    },
    {
      "metadata": {
        "id": "uNpSUsQX819C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45b37953-78ec-4c26-aefa-5a66624f85a9"
      },
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7188, 28) (7188, 2)\n",
            "(3541, 28) (3541, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-kY4eR4K9i5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "2ae7f3b6-09bc-402e-c470-351f5dada636"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            " - 18s - loss: 0.4384 - acc: 0.8171\n",
            "Epoch 2/25\n",
            " - 16s - loss: 0.3234 - acc: 0.8645\n",
            "Epoch 3/25\n",
            " - 16s - loss: 0.2775 - acc: 0.8840\n",
            "Epoch 4/25\n",
            " - 16s - loss: 0.2524 - acc: 0.8952\n",
            "Epoch 5/25\n",
            " - 16s - loss: 0.2259 - acc: 0.9094\n",
            "Epoch 6/25\n",
            " - 16s - loss: 0.2079 - acc: 0.9167\n",
            "Epoch 7/25\n",
            " - 16s - loss: 0.1939 - acc: 0.9233\n",
            "Epoch 8/25\n",
            " - 16s - loss: 0.1723 - acc: 0.9327\n",
            "Epoch 9/25\n",
            " - 16s - loss: 0.1517 - acc: 0.9391\n",
            "Epoch 10/25\n",
            " - 16s - loss: 0.1440 - acc: 0.9406\n",
            "Epoch 11/25\n",
            " - 16s - loss: 0.1362 - acc: 0.9427\n",
            "Epoch 12/25\n",
            " - 16s - loss: 0.1225 - acc: 0.9498\n",
            "Epoch 13/25\n",
            " - 16s - loss: 0.1230 - acc: 0.9517\n",
            "Epoch 14/25\n",
            " - 16s - loss: 0.1098 - acc: 0.9538\n",
            "Epoch 15/25\n",
            " - 16s - loss: 0.1118 - acc: 0.9540\n",
            "Epoch 16/25\n",
            " - 16s - loss: 0.1011 - acc: 0.9610\n",
            "Epoch 17/25\n",
            " - 16s - loss: 0.0971 - acc: 0.9602\n",
            "Epoch 18/25\n",
            " - 16s - loss: 0.1018 - acc: 0.9592\n",
            "Epoch 19/25\n",
            " - 16s - loss: 0.0934 - acc: 0.9622\n",
            "Epoch 20/25\n",
            " - 16s - loss: 0.0873 - acc: 0.9630\n",
            "Epoch 21/25\n",
            " - 16s - loss: 0.0880 - acc: 0.9633\n",
            "Epoch 22/25\n",
            " - 16s - loss: 0.0825 - acc: 0.9656\n",
            "Epoch 23/25\n",
            " - 16s - loss: 0.0816 - acc: 0.9651\n",
            "Epoch 24/25\n",
            " - 16s - loss: 0.0839 - acc: 0.9644\n",
            "Epoch 25/25\n",
            " - 16s - loss: 0.0885 - acc: 0.9638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fecd89c44a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "gCVsIvj5_ZeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extracting a validation set, and measuring score and accuracy."
      ]
    },
    {
      "metadata": {
        "id": "2UAsL2T59rso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17e59590-03ac-4890-af8f-fc189e87f349"
      },
      "cell_type": "code",
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 0.74\n",
            "acc: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IrBG5Q4Y_dhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2HAuNLH_miY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "EDIT : After playing around with the model for a while, It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not up to the mark really. \n",
        "\n",
        "It is mainly because he positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweet "
      ]
    },
    {
      "metadata": {
        "id": "1wuvC_Lx_89C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "761841da-041e-416a-f8fe-0de8c2e7ab15"
      },
      "cell_type": "code",
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_acc 57.60517799352751 %\n",
            "neg_acc 89.50461796809404 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QlEmI39QAQTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As is evident from the above result, my model can very nicely classify a negative tweet but has problems with positive tweets after testing and cross verifying my model over the validation set\n",
        "\n",
        "To improve, we need \n",
        "\n",
        "\n",
        "*   More Positive tweet samples\n",
        "*   Some more GPU to tackle that data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1Brif6XXA0_3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Trying out on custom tweets\n"
      ]
    },
    {
      "metadata": {
        "id": "wB5twc-mA-4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "086d862a-11df-404e-a706-34aa49e1ebb1"
      },
      "cell_type": "code",
      "source": [
        "twt = ['what the fuck']\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0  48   1 543]]\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CFTBu_NgB8ll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#END NOTES :-\n",
        "Don't use my data. I used it to do things fast and easy. It is very imbalanced.\n",
        "\n",
        "Use your own twitter data that you mailed me. Use my LSTM architecture and my methodology. It's industry standard and falls into the category of Deep learning.\n",
        "\n",
        "Regression for NLP is outdated and not standard"
      ]
    },
    {
      "metadata": {
        "id": "9MAhhAq0Bu0O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}